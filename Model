import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
from matplotlib import pyplot
pyplot.rcParams['figure.dpi'] = 600
pyplot.rcParams['savefig.dpi'] = 600
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense, RepeatVector, TimeDistributed, LayerNormalization
from keras.callbacks import Callback
import tensorflow as tf
from sklearn.metrics import r2_score
import talib
import ta
import time
import pandas as pd
import pandas_ta as pta
from joblib import Parallel, delayed
from keras.layers import Layer
import keras.backend as K

# Set the precision to a higher value (e.g., 15 decimal places)
pd.set_option('display.precision', 15)

physical_devices = tf.config.list_physical_devices('GPU')

def build_model(lookback, n_features):
    model = Sequential()  

    model.add(LSTM(units=128, return_sequences=True, input_shape=(lookback, n_features)))
    
    # Reshape X_train before RepeatVector layer
    model.add(LSTM(units=128, return_sequences=False))  # Add this line
    model.add(RepeatVector(lookback))  

    model.add(LSTM(units=128, return_sequences=True))
    model.add(Dense(units=64))
    model.add(Dense(units=32))
    model.add(Dense(units=16))

    model.add(TimeDistributed(Dense(units=1)))

    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)  

    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])  

    return model

def calculate_weighted_alpha(data, window_length=14):
    close = data['Close']
    high = data['High']
    low = data['Low']

    change = close.diff()
    positive_changes = change.copy()
    positive_changes[positive_changes < 0] = 0

    negative_changes = change.copy()
    negative_changes[negative_changes > 0] = 0

    average_positive = positive_changes.rolling(window_length).sum()
    average_negative = np.abs(negative_changes.rolling(window_length).sum())

    weighted_alpha = (100 * average_positive) / (average_positive + average_negative)
    return weighted_alpha

def calculate_rsi(data, window_length=14):
    delta = data.diff()
    up, down = delta.copy(), delta.copy()
    up[up < 0] = 0
    down[down > 0] = 0

    average_gain = up.rolling(window_length).mean()
    average_loss = abs(down.rolling(window_length).mean())

    rs = average_gain / average_loss
    rsi = 100 - (100 / (1 + rs))

    return rsi

def calculate_fisher_transform(data):
    close = data['Close']
    x = (close - close.min()) / (close.max() - close.min())
    fisher_transform = 0.5 * np.log((1 + x) / (1 - x))
    return fisher_transform

def calculate_ker(data, window_length=10):
    change = np.abs(data.diff())
    volatility = pd.Series(change.rolling(window_length).sum(), name='Volatility')
    roc = data.diff(window_length - 1) / data.shift(window_length - 1)
    ker = roc / volatility
    ker = (ker + 1) / 2  # Scale to range [0, 1]
    return ker

def calculate_keltner_bands(data):
    bands = ta.volatility.keltner_channel_hband_indicator(data['High'], data['Low'], data['Close'])
    return bands

def calculate_supertrend(data):
    supertrend = pta.supertrend(data['High'], data['Low'], data['Close'])
    return supertrend['SUPERT_7_3.0']

def load_stock_data(ticker, unit='1m'):
    data = yf.download(ticker, interval=str(unit), progress=False)
    print(data)

    if data.empty:
        print(f"No data available for ticker {ticker} with interval {unit}")
        return None

    # Define the list of features to calculate
    features_to_calculate = [
        'RSI', 'Fisher_Transform', 'Bollinger_Bands', 'CCI', 'ROC', 'Stoch_RSI_K', 'BOP', 'DMI',
        'Typical_Price', 'Ultimate_Oscillator', 'Adaptive_Center_of_Gravity', 'KER', 'HT_TRENDMODE',
        'HT_DCPHASE', 'ATR', 'Supertrend', 'Weighted_Alpha'
    ]

    # Define the function to calculate a single feature
    def calculate_feature(data, feature_name):
        open_price = data['Open']
        high = data['High']
        low = data['Low']
        close = data['Close']
        volume = data['Volume']
        if feature_name == 'RSI':
            return calculate_rsi(close)
        elif feature_name == 'Fisher_Transform':
            return calculate_fisher_transform({'Close': close})
        elif feature_name == 'Bollinger_Bands':
            return talib.BBANDS(close)[0]
        elif feature_name == 'CCI':
            return talib.CCI(high, low, close)
        elif feature_name == 'ROC':
            return talib.ROC(close)
        elif feature_name == 'Stoch_RSI_K':
            return talib.STOCHRSI(close)[0]
        elif feature_name == 'BOP':
            return talib.BOP(open_price, high, low, close)
        elif feature_name == 'DMI':
            return talib.DX(high, low, close)
        elif feature_name == 'Typical_Price':
            return (high + low + close) / 3
        elif feature_name == 'Ultimate_Oscillator':
            return talib.ULTOSC(high, low, close)
        elif feature_name == 'Adaptive_Center_of_Gravity':
            return talib.ADOSC(high, low, close, volume)
        elif feature_name == 'KER':
            return calculate_ker(close)
        elif feature_name == 'HT_TRENDMODE':
            return ta.trend.sma_indicator(close, window=9)
        elif feature_name == 'HT_DCPHASE':
            return talib.HT_DCPHASE(close)
        elif feature_name == 'ATR':
            return ta.volatility.average_true_range(high, low, close)
        elif feature_name == 'Supertrend':
            return calculate_supertrend(data)
        elif feature_name == 'Weighted_Alpha':
            return calculate_weighted_alpha(data)
        elif feature_name == 'Keltner_Bands':
            return calculate_keltner_bands(data)

    # Calculate features in parallel using multiple cores
    calculated_features = Parallel(n_jobs=-1)(delayed(calculate_feature)(data, feature) for feature in features_to_calculate)

    # Combine the calculated features into the data DataFrame
    data_values = np.array(data)

    for i, feature_name in enumerate(features_to_calculate):
        feature_values = calculated_features[i]
        # Check that the feature values have the correct shape
        if feature_values.shape[0] != data_values.shape[0]:
            raise ValueError(f"Feature {feature_name} has incorrect shape: {feature_values.shape}")
        data_values = np.column_stack((data_values, feature_values))

    data_values = data_values[~np.isnan(data_values).any(axis=1)]  # remove rows with NaN

    return data_values

def prepare_data(data_values, lookback, data):
    prices = data_values[:, 0].reshape(-1, 1)  # Assuming the close price is in the first column

    # Remove infinite and extremely large values
    prices = np.where(np.isfinite(prices), prices, np.nan)
    max_value = np.nanmax(prices)
    finite_mask = np.abs(prices) < 1e10 * max_value
    prices = np.where(finite_mask, prices, np.nan)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    prices = scaler.fit_transform(prices)

    # Prepare the data
    X = []
    y = []
    for i in range(lookback, len(prices)):
        X.append(prices[i-lookback:i, 0])
        y.append(prices[i, 0])
    X = np.array(X)
    y = np.array(y)

    return X, y, scalers_x, scaler_y

def split_data(X, y, train_size=0.6, val_size=0.3):
    n = len(X)
    
    X_train = X[:int(n * train_size)]
    y_train = y[:int(n * train_size)]
    
    X_val = X[int(n * train_size):int(n * (train_size + val_size))]
    y_val = y[int(n * train_size):int(n * (train_size + val_size))]
    
    X_test = X[int(n * (train_size + val_size)):]
    y_test = y[int(n * (train_size + val_size)):]
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def calculate_mape(y_true, y_pred):
    epsilon = 1e-2
    absolute_errors = np.abs(y_true - y_pred)
    percentage_errors = absolute_errors / np.abs(y_true + epsilon)
    mape = np.mean(percentage_errors) * 100
    return mape

def train_model(X_train, y_train, X_val, y_val, lookback, ticker):
    threshold_value = 0.01
    num_crossings = 5

    model = build_model(lookback, X_train.shape[2])
    X_train_reshaped = X_train.reshape(X_train.shape[0], lookback, X_train.shape[2])
    X_val_reshaped = X_val.reshape(X_val.shape[0], lookback, X_val.shape[2])
    y_train = y_train.reshape(-1, 1)
    y_val = y_val.reshape(-1, 1)

    class CustomEarlyStopping(Callback):
        def __init__(self, threshold, num_crossings):
            super(CustomEarlyStopping, self).__init__()
            self.threshold = threshold
            self.num_crossings = num_crossings
            self.crossings_count = 0
            self.val_mae = []

        def on_epoch_end(self, epoch, logs=None):
            val_mae = logs.get('val_mean_absolute_error')
            self.val_mae.append(val_mae)

            if val_mae < self.threshold:
                self.crossings_count += 1
                print(f'\nCrossed below threshold {self.crossings_count} times')

                if self.crossings_count >= self.num_crossings:
                    print(f'\nStopping training: validation MAE crossed below threshold {self.num_crossings} times')
                    self.model.stop_training = True

    custom_early_stopping = CustomEarlyStopping(threshold=threshold_value, num_crossings=num_crossings)

    # Then in your callbacks:
    class R2ScoreCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            val_predictions = self.model.predict(X_val_reshaped)
            val_predictions = val_predictions[:, -1, :]  # Add this line
            r2 = r2_score(y_val, val_predictions)
            print(f'Validation R2 Score: {r2}')

    class MAPECallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            val_predictions = self.model.predict(X_val_reshaped)
            val_predictions = val_predictions[:, -1, :]  # Add this line
            mape = calculate_mape(y_val, val_predictions)
            print(f'Validation MAPE: {mape}%')

    history = model.fit(X_train_reshaped, y_train, epochs=60, batch_size=16,
                        validation_data=(X_val_reshaped, y_val),
                        callbacks=[custom_early_stopping, R2ScoreCallback(), MAPECallback()], verbose=1)

    training_predicted = model.predict(X_train_reshaped)
    validation_predicted = model.predict(X_val_reshaped)

    return model, training_predicted, validation_predicted, history

def plot_data(start, end, data, color, label):
    plt.plot(np.arange(start, end), data, color=color, label=label)

def plot_performance(y_train, training_predicted, y_val, validation_predicted, y_test, test_predicted, scaler_y, lookback):
    plt.figure(figsize=(20, 10))

    # Plotting historical prices
    historical_prices_train = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()
    plot_data(0, len(historical_prices_train), historical_prices_train, 'blue', 'Actual Prices (Training)')
    print("Start Time Step - Actual Prices (Training):", 0)  # Print the starting time step

    historical_prices_val = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()
    plot_data(len(historical_prices_train), len(historical_prices_train) + len(historical_prices_val),
              historical_prices_val, 'orange', 'Actual Prices (Validation)')
    print("Start Time Step - Actual Prices (Validation):", len(historical_prices_train))  # Print the starting time step

    # Plotting predicted prices
    predicted_prices_train = scaler_y.inverse_transform(training_predicted[:, -1, :]).flatten()
    plot_data(0, len(predicted_prices_train),
              predicted_prices_train, 'green', 'Predicted Prices (Training)')
    print("Start Time Step - Predicted Prices (Training):", 0)  # Print the starting time step

    predicted_prices_val = scaler_y.inverse_transform(validation_predicted[:, -1, :]).flatten()
    plot_data(len(historical_prices_train), len(historical_prices_train) + len(predicted_prices_val),
              predicted_prices_val, 'red', 'Predicted Prices (Validation)')
    print("Start Time Step - Predicted Prices (Validation):",
          len(historical_prices_train))  # Print the starting time step

    # Plotting actual prices of testing data
    historical_prices_test = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    plot_data(len(historical_prices_train) + len(historical_prices_val),
              len(historical_prices_train) + len(historical_prices_val) + len(historical_prices_test),
              historical_prices_test, 'purple', 'Actual Prices (Testing)')
    print("Start Time Step - Actual Prices (Testing):",
          len(historical_prices_train) + len(historical_prices_val))  # Print the starting time step

    # Plotting predicted prices of testing data
    predicted_prices_test = scaler_y.inverse_transform(test_predicted[:, -1, :]).flatten()
    plot_data(len(historical_prices_train) + len(historical_prices_val),
              len(historical_prices_train) + len(historical_prices_val) + len(predicted_prices_test),
              predicted_prices_test, 'yellow', 'Predicted Prices (Testing)')
    print("Start Time Step - Predicted Prices (Testing):",
          len(historical_prices_train) + len(historical_prices_val))  # Print the starting time step

    plt.title('Stock Price Prediction')
    plt.xlabel('Time Step')
    plt.ylabel('Price')
    plt.legend(loc='best')
    plt.show()

def plot_loss(history):
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    plt.figure(figsize=(10, 6))
    plt.plot(train_loss, color='blue', label='Training Loss')
    plt.plot(val_loss, color='red', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

def make_predictions(X_train, y_train, X_test, y_test, lookback, model, scalers_x, scaler_y, prediction_length, ticker):
    # Download the latest data for the ticker
    data = load_stock_data(ticker)
    print(f"Data for {ticker}:")
    print(data)

    if data is None:
        return

    X_latest, _, scalers_x_latest, _ = prepare_data(data, lookback)

    # Reshape X_latest
    X_latest_reshaped = X_latest.reshape(X_latest.shape[0], lookback, X_latest.shape[2])

    # Use the latest sequence for predictions
    last_sequence = X_latest_reshaped[-1:, :, :]

    # Pre-allocate the predictions array
    predictions = np.empty(prediction_length)

    for i in range(prediction_length):
        last_sequence_reshaped = np.reshape(last_sequence, (1, lookback, X_latest_reshaped.shape[2]))
        predicted_price = model.predict(last_sequence_reshaped)
        predicted_price_scalar = predicted_price[0, -1, 0]  # Access the last predicted value

        # Prepare the input for the next prediction
        next_input = last_sequence[:, 1:, :]

        # Get the most recent data from the ticker,
        # but replace the 'Open' price with the predicted value
        most_recent_data = X_latest[-1, -1, :].copy()
        most_recent_data[0] = predicted_price_scalar
        most_recent_data_combined = most_recent_data.reshape(1, 1, -1)

        next_input = np.concatenate((next_input, most_recent_data_combined), axis=1)

        last_sequence = next_input
        predictions[i] = predicted_price_scalar

    # Scale back the predicted and actual prices to the original scale
    predicted_prices = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()

    # Get the historical prices for plotting
    historical_prices = data[-100:, 3]  # Assuming 'Close' price is at index 3 and considering the last 100 time steps

    # Plot the predicted prices
    plt.figure(figsize=(20, 10))
    plt.plot(np.arange(len(historical_prices), len(historical_prices) + prediction_length), predicted_prices, color='red', label='Predicted Prices')
    plt.plot(np.arange(len(historical_prices)), historical_prices, color='blue', label='Historical Prices')
    plt.title('Stock Price Prediction')
    plt.xlabel('Time Step')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

def predict_stock_price(ticker, lookback, unit, prediction_length):
    start_time = time.time()  # Start the timer

    data = load_stock_data(ticker, unit)

    if data is None:
        return

    X, y, scalers_x, scaler_y = prepare_data(data, lookback, data)

    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)

    # Reshape y arrays
    y_train = y_train.reshape(-1, 1)
    y_val = y_val.reshape(-1, 1)
    y_test = y_test.reshape(-1, 1)

    model, training_predicted, validation_predicted, history = train_model(X_train,
                                                                          y_train,
                                                                          X_val,
                                                                          y_val,
                                                                          lookback,
                                                                          ticker)

    test_predicted = model.predict(X_test)  # Predict on the testing set

    plot_performance(y_train,
                     training_predicted,
                     y_val,
                     validation_predicted,
                     y_test,
                     test_predicted,
                     scaler_y,
                     lookback)  # Pass test_predicted as an argument

    plot_loss(history)

    make_predictions(X_train,
                     y_train,
                     X_test,
                     y_test,
                     lookback,
                     model,
                     scalers_x,
                     scaler_y,
                     prediction_length,
                     ticker)

    end_time = time.time()  # End the timer
    elapsed_time = end_time - start_time  # Calculate the elapsed time
    print(f'Time elapsed for prediction: {elapsed_time} s')

# Call the function with desired parameters
prediction_length = 50
predict_stock_price('AAPL', 120, '1d', prediction_length)
